{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iTj_lCCPhYt",
        "outputId": "1364f919-2baa-463f-ced8-bad142141bc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Aug 14 04:12:18 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yq9gsm9AQBdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from io import BytesIO\n",
        "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torch, gc\n",
        "import glob\n",
        "import os\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import IPython.display as display\n",
        "\n",
        "def load_image(image_path, x32=False):\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    if x32:\n",
        "        def to_32s(x):\n",
        "            return 256 if x < 256 else x - x % 32\n",
        "        w, h = img.size\n",
        "        img = img.resize((to_32s(w), to_32s(h)))\n",
        "\n",
        "    return img\n",
        "# input_path = \"/content/input2/*\"       create a new folder for input image and another for output\n",
        "input_path = \"/content/input/*\"\n",
        "o_path = \"/content/output\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# model = torch.hub.load(\"bryandlee/animegan2-pytorch:main\", \"generator\", device=device).eval()\n",
        "# face2paint = torch.hub.load(\"bryandlee/animegan2-pytorch:main\", \"face2paint\", device=device)\n",
        "# model = torch.hub.load(\"bryandlee/animegan2-pytorch:main\", \"generator\", pretrained=\"paprika\", device=device)\n",
        "model = torch.hub.load(\"bryandlee/animegan2-pytorch:main\", \"generator\", pretrained=\"face_paint_512_v2\", device=device)\n",
        "\n",
        "\n",
        "\n",
        "file_list = glob.glob(input_path)\n",
        "print(file_list)\n",
        "for image in file_list:\n",
        "  img_name = os.path.basename(image)\n",
        "  print(img_name)\n",
        "  image = load_image(image)\n",
        "  with torch.no_grad():\n",
        "    image = to_tensor(image).unsqueeze(0) * 2 - 1\n",
        "    out = model(image.to(device), False).cpu()\n",
        "    out = out.squeeze(0).clip(-1, 1) * 0.5 + 0.5\n",
        "    out = to_pil_image(out)\n",
        "    output_path = os.path.join(o_path,img_name)\n",
        "    out.save(output_path)\n",
        "    print(f\"image saved: {output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2_ts3LEQBfv",
        "outputId": "7b05fa4c-d75d-4e8b-c161-f83c1f3c0d1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/bryandlee_animegan2-pytorch_main\n",
            "Downloading: \"https://github.com/bryandlee/animegan2-pytorch/raw/main/weights/face_paint_512_v2.pt\" to /root/.cache/torch/hub/checkpoints/face_paint_512_v2.pt\n",
            "100%|██████████| 8.20M/8.20M [00:00<00:00, 176MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/input/b2.png']\n",
            "b2.png\n",
            "image saved: /content/output/b2.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RxB5FLcQj9v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zq7ZoorZj9x8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AZEiH8Fbj91A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YOH9BpwVj94I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2c9U7-naj964"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U0Kjev5Rj99o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3vfHGCvQj-Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kMMIBMHmj-DR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7XGYOMGyj-GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XUjgw-dGQBiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install torch torchvision numpy\n",
        "!pip install git+https://github.com/openai/CLIP.git\n"
      ],
      "metadata": {
        "id": "Xmk4cncSQBlK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4f617c3-9048-4878-ac06-63fdf37608d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-5i8bhahb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-5i8bhahb\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.15.2+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.27.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "import clip\n"
      ],
      "metadata": {
        "id": "PjcPsfQ0QBnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXRY3ElBkTr-",
        "outputId": "ffdd8f98-b8c6-42d9-e739-b33897e11519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:07<00:00, 47.3MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_image = Image.open(\"/content/pexels-анна-галашева-5742750.jpg\")  # Provide the path to your image\n"
      ],
      "metadata": {
        "id": "cNRB-rlgkZIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "# Define your stable diffusion model here\n",
        "def your_diffusion_step(input_frame):\n",
        "    # Replace this with your actual diffusion process\n",
        "    # For this example, let's add Gaussian noise to the input frame\n",
        "    noise = torch.randn_like(input_frame) * 0.1  # Adjust noise level as needed\n",
        "    new_frame = input_frame + noise\n",
        "    return new_frame\n",
        "\n",
        "# Load the input image\n",
        "input_image = Image.open(\"/content/pexels-анна-галашева-5742750.jpg\")  # Provide the path to your image\n",
        "\n",
        "# Preprocess the input image\n",
        "preprocess = Compose([Resize((256, 256)), ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "input_image = preprocess(input_image).unsqueeze(0)\n",
        "\n",
        "# Generate animation frames using your stable diffusion model\n",
        "num_frames = 20  # Adjust this based on your needs\n",
        "animation_frames = []\n",
        "\n",
        "current_frame = input_image\n",
        "for _ in range(num_frames):\n",
        "    new_frame = your_diffusion_step(current_frame)  # Replace with actual diffusion step\n",
        "\n",
        "    animation_frames.append(to_pil_image(new_frame[0]))\n",
        "    current_frame = new_frame\n",
        "\n",
        "# Save the animation frames\n",
        "for i, frame in enumerate(animation_frames):\n",
        "    frame.save(f\"animation_frame_{i}.png\")\n",
        "\n"
      ],
      "metadata": {
        "id": "v3RWjxHAk5Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_animation_frames(input_image):\n",
        "    # Define your stable diffusion model here\n",
        "    # This function should generate a sequence of animation frames given an input image\n",
        "\n",
        "    # For demonstration purposes, let's assume you have a loop that generates frames\n",
        "    num_frames = 20  # Adjust this based on your needs\n",
        "    animation_frames = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        current_frame = input_image\n",
        "        for _ in range(num_frames):\n",
        "            # Perform diffusion step to generate a new frame\n",
        "            new_frame = your_diffusion_step(current_frame)  # Replace with actual diffusion step\n",
        "\n",
        "            # Append the new frame to the list of animation frames\n",
        "            animation_frames.append(new_frame)\n",
        "\n",
        "            # Update the current frame for the next iteration\n",
        "            current_frame = new_frame\n",
        "\n",
        "    return animation_frames\n",
        "\n",
        "# Load the input image\n",
        "input_image = Image.open(\"/content/pexels-furknsaglam-4636160.jpg\")  # Provide the path to your image\n",
        "\n",
        "# Preprocess the input image\n",
        "input_image = preprocess(input_image).unsqueeze(0).to(device)\n",
        "\n",
        "# Generate animation frames using your stable diffusion model\n",
        "animation_frames = generate_animation_frames(input_image)\n",
        "\n",
        "# Save the animation frames\n",
        "for i, frame in enumerate(animation_frames):\n",
        "    frame_image = frame\n",
        "    frame_image.save(f\"animation_frame_{i}.png\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "Io7h0USrkxHy",
        "outputId": "9744fe9b-d32b-481a-b493-53684409f198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-a35eb68b2ad1>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manimation_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mframe_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mframe_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"animation_frame_{i}.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'save'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "num_frames = 20\n",
        "\n",
        "frames = []\n",
        "for i in range(num_frames):  # Adjust 'num_frames' as needed\n",
        "    frame = Image.open(f\"animation_frame_{i}.png\")\n",
        "    frames.append(frame)\n",
        "\n",
        "imageio.mimsave(\"animation3.gif\", frames, duration=0.15)  # Adjust duration as needed\n"
      ],
      "metadata": {
        "id": "1dY3pN2-kxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WqxRZRvIkxNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dzy5d8Papdgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CY823Pscpdd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "\n",
        "# Load content and style images\n",
        "content_image = Image.open(\"/content/pexels-furknsaglam-4636160.jpg\")\n",
        "style_image = Image.open(\"style.jpg\")\n",
        "\n",
        "# Preprocess images for VGG19 model\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(512),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],      std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "content_tensor = preprocess(content_image).unsqueeze(0)\n",
        "style_tensor = preprocess(style_image).unsqueeze(0)\n",
        "\n",
        "# Load a pre-trained VGG19 model\n",
        "vgg = models.vgg19(pretrained=True).features\n",
        "vgg.to(\"cuda\").eval()\n",
        "\n",
        "# Define layers for style and content extraction\n",
        "content_layers = [\"conv_4\"]\n",
        "style_layers = [\"conv_1\", \"conv_2\", \"conv_3\", \"conv_4\", \"conv_5\"]\n",
        "\n",
        "# Initialize the generated image as a copy of content image\n",
        "generated_image = content_tensor.clone().requires_grad_(True).to(\"cuda\")\n",
        "\n",
        "# Hyperparameters\n",
        "style_weight = 1000000\n",
        "content_weight = 1\n",
        "lr = 0.01\n",
        "steps = 2000\n",
        "\n",
        "optimizer = optim.Adam([generated_image], lr=lr)\n",
        "\n",
        "# Style transfer process\n",
        "for step in range(steps):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Pass the generated image through the VGG19 model\n",
        "    model_output = vgg(generated_image)\n",
        "\n",
        "    content_loss = 0\n",
        "    style_loss = 0\n",
        "\n",
        "    for layer_name, target_content, target_style in zip(content_layers, content_tensor, style_tensor):\n",
        "        layer = model_output[layer_name]\n",
        "\n",
        "        content_loss += torch.mean((layer - target_content.to(\"cuda\")) ** 2)\n",
        "\n",
        "        _, c, h, w = layer.shape\n",
        "        layer = layer.view(c, h * w)\n",
        "        target_style = target_style.view(c, h * w)\n",
        "\n",
        "        gram_matrix = torch.mm(layer, layer.t())\n",
        "        target_gram_matrix = torch.mm(target_style, target_style.t())\n",
        "\n",
        "        style_loss += torch.mean((gram_matrix - target_gram_matrix.to(\"cuda\")) ** 2)\n",
        "\n",
        "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(f\"Step [{step}/{steps}], Loss: {total_loss.item()}\")\n",
        "\n",
        "# Convert the generated image tensor back to a PIL image\n",
        "generated_image = generated_image.cpu().detach().squeeze(0)\n",
        "generated_image = transforms.ToPILImage()(generated_image)\n",
        "\n",
        "# Save the final generated image\n",
        "generated_image.save(\"generated_image.jpg\")\n"
      ],
      "metadata": {
        "id": "omQC0Xb2pdbc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}